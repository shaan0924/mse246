---
title: "Project Code"
output: github_document
authors: "Jack Parkin, Sihguat Torres, Shaan Patel, Isaac Arocha"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE, message=FALSE}
# Used Throughout
library(lubridate)
library(quantmod)
library(tidyverse)
library(data.table)
library(readxl) 
library(here) # Instead of setting WD with setwd, use the here() function for code transferability between devices

# For Logistic Model
library(leaps)
library(ROCR)
library(glmnet)

# For Neural Net Model
library(neuralnet)

#Extra
library(devtools)
```

```{r}
# NOT WORKING - meant to provide us access to the Medical Expenditure Panel 
# Survey Household Component Dataset. But the package installation process isn't working 
# install_github("e-mitchell/meps_r_pkg/MEPS")
# library(MEPS)
```


# Data Import 
## ONLY RUN ON FIRST PASS
```{r warning=FALSE, message=FALSE}
# raw_data_xlxs <- read_xlsx(here('SBA Loan data .xlsx'))
# write.csv(raw_data_xlxs,here("SBA Loan data .csv"), row.names = FALSE)
```

## All Other Data
```{r warning=FALSE, message=FALSE}
raw_data <- read.csv("SBA Loan data .csv", header = TRUE)

#GDP Import
GDP = read.csv("GDP.csv")

#SP500 Import
SP500 = getSymbols("^GSPC",from = "1990-01-31",to = "2014-12-31", periodicity = 'monthly', auto.assign = FALSE)

#FedFunds Import
FEDFUNDS = read.csv("FEDFUNDS.csv")

#CPI Import (Consumer Price Index for All Urban Consumers: All Items in U.S. City Average)
CPI = read.csv("CPIAUCSL.csv")

#Unemployment
UnemploymentUSbyState = read.csv('StateUR.csv', header = TRUE)
```


# Data Exploration

## Gross Approval Frequencies and Annual Default Rate over time by Gross Approval Size

```{r}
temp = raw_data
temp = 
  transform(
    raw_data, 
    bin = 
      cut(
        GrossApproval, 
        breaks = c(0,100000, 250000, 500000, 750000, 1000000, 2000000, 10000000))
    )

raw_data$Size[temp$bin == "(0,1e+05]"] = "<$100k"
raw_data$Size[temp$bin == "(1e+05,2.5e+05]"] = "$100k-250K"
raw_data$Size[temp$bin == "(2.5e+05,5e+05]"] = "$250k-500k"
raw_data$Size[temp$bin == "(5e+05,7.5e+05]"] = "$500k-750k"
raw_data$Size[temp$bin == "(7.5e+05,1e+07]"] = "$750k+"
raw_data$Size[temp$bin == "(7.5e+05,1e+06]"] = "$750k-$1M"
raw_data$Size[temp$bin == "(1e+06,2e+0+6]"] = "$1M-$2M"
raw_data$Size[temp$bin == "(2e+06,1e+07]"] = "$2M+"

raw_data %>% 
  drop_na(Size) %>% 
  group_by(Size) %>% 
  summarise("Volume" = sum(GrossApproval)) %>% 
  mutate(Size = factor(Size, c("<$100k", "$100k-250K", "$250k-500k", "$500k-750k", "$750k-$1M", "$2M+"))) %>% 
  ggplot(aes(x = Size, y = Volume, fill = Size)) + 
  geom_bar(stat = "identity") + 
  ggtitle("Figure 1: Total Loan Volume by Loan Size") + 
  xlab("Loan Size") + 
  ylab("Total Loan Volume")
```

Figure 1 shows the frequency distribution over loan size in the SBA Loan Data. The vast majority of loan volume here came from loans between \$100k and \$1M, with loan volumes from loans over \$2m and under \$100k staying relatively small. Loans under \$100k, likely personal loans, makesup an especially small amount of the overall loan volume.

```{r}
raw_data %>% 
  group_by(ApprovalFiscalYear, Size) %>% 
  summarise(n = sum(n())) %>% 
  mutate(Size = factor(Size, levels = c("<$100K", "$100k-250K", "$250k-500k", "$500k-750k", "$750k+", "$750k-$1M", "$1M-$2M", "$2M+"))) %>% 
  drop_na() %>% 
  ggplot(
    aes(
      x = ApprovalFiscalYear,
      y = n,
      group = Size,
      color = Size
    )
  ) +
  geom_point() +
  geom_line() +
  scale_y_continuous(name = "Number of Loans Approved", breaks = seq(0, 3500, 500)) +
  ggtitle("Figure 2: Loans Approved by Approval Year") +
  xlab("Approval Year")

raw_data %>% 
  group_by(ApprovalFiscalYear, Size) %>% 
  summarise("Annual Default Rate" = sum(LoanStatus == "CHGOFF")/sum(LoanStatus == "CHGOFF" | LoanStatus == "PIF")) %>% 
  mutate(Size = factor(Size, levels = c("<$100K", "$100k-250K", "$250k-500k", "$500k-750k", "$750k+", "$750k-$1M", "$1M-$2M", "$2M+"))) %>% 
  drop_na(Size) %>% 
  ggplot(
    aes(
      x = ApprovalFiscalYear, 
      y = `Annual Default Rate`, 
      group = Size, color = Size)
    ) + 
  geom_point() + 
  geom_line() + 
  ggtitle("Figure 3: Approval Year vs. Annual Default Rate by Gross Approval Size") + 
  xlab("Approval Year") + 
  ylab("Annual Default Rate") +
  theme_light()
```

Not shown in Figure 1 is the timing of these loans. In Figure 2, this dimension is explored, and it's clear that the loan frequencies spiked in the second half of the 1990s, in the prelude to the Great Financial Crisis, and during the recovery post-GFC. Figure 3 shows that the default rates varied tremendously over time, and the GFC stands out for its exceptionally high default rates among all loan sizes in the dataset. 

## Borrower Region vs. Annual Default Rate

```{r}
raw_data$BorrowerRegion[raw_data$BorrState == "ME"| raw_data$BorrState == "VT"| raw_data$BorrState == "NH"|raw_data$BorrState == "MA"| raw_data$BorrState == "RI"| raw_data$BorrState == "NY"| raw_data$BorrState == "CT"| raw_data$BorrState == "NJ"|raw_data$BorrState == "PA"] = "Northeast"

raw_data$BorrowerRegion[raw_data$BorrState == "ND"| raw_data$BorrState == "SD"| raw_data$BorrState == "NE"|raw_data$BorrState == "KS"| raw_data$BorrState == "MN"| raw_data$BorrState == "IA"| raw_data$BorrState == "MO"| raw_data$BorrState == "WI" | raw_data$BorrState == "IL"| raw_data$BorrState == "IN"| raw_data$BorrState == "MI" | raw_data$BorrState == "OH"] = "Midwest"

raw_data$BorrowerRegion[raw_data$BorrState == "TX"| raw_data$BorrState == "OK"| raw_data$BorrState == "AR"|raw_data$BorrState == "LA"| raw_data$BorrState == "MS"| raw_data$BorrState == "AL"| raw_data$BorrState == "TN"| raw_data$BorrState == "KY"| raw_data$BorrState == "WV"| raw_data$BorrState == "VA"|raw_data$BorrState == "MD"| raw_data$BorrState == "DC"| raw_data$BorrState == "DE"| raw_data$BorrState == "NC"| raw_data$BorrState == "SC"| raw_data$BorrState == "GA" | raw_data$BorrState == "FL" | raw_data$BorrState == "PR" | raw_data$BorrState == "VI"] = "South"

raw_data$BorrowerRegion[raw_data$BorrState == "WA"| raw_data$BorrState == "OR"| raw_data$BorrState == "CA"|raw_data$BorrState == "NV"| raw_data$BorrState == "ID"| raw_data$BorrState == "MT"| raw_data$BorrState == "UT"| raw_data$BorrState == "WY"|raw_data$BorrState == "CO"| raw_data$BorrState == "NM"| raw_data$BorrState == "AZ"| raw_data$BorrState == "AK"| raw_data$BorrState == "HI" | raw_data$BorrState == "GU"] = "West"

raw_data %>% 
  drop_na(BorrowerRegion) %>% 
  group_by(BorrowerRegion) %>% 
  summarise("Volume" = sum(GrossApproval)) %>% 
  ggplot(aes(x = BorrowerRegion, y = Volume, color = BorrowerRegion)) + 
  geom_bar(stat = "identity") + 
  ggtitle("Total Loan Volume by Borrower Region") + 
  xlab("Borrower Region") + 
  ylab("Total Loan Volume")

raw_data %>% 
  drop_na(BorrowerRegion) %>%  
  group_by(ApprovalFiscalYear, BorrowerRegion) %>% 
  summarise("Annual Default Rate" = sum(LoanStatus == "CHGOFF")/sum(LoanStatus == "CHGOFF" | LoanStatus == "PIF")) %>% 
  ggplot(
    aes(
      x = ApprovalFiscalYear, 
      y = `Annual Default Rate`, 
      group = BorrowerRegion, 
      color = BorrowerRegion
    )
  ) + 
  geom_point() + 
  geom_line() + 
  scale_y_continuous(breaks = seq(0, .7, .1)) +
  ggtitle("Figure 4: Approval Year vs. Annual Default Rate by Borrower Region") + 
  xlab("Approval Year") + 
  ylab("Annual Default Rate") +
  theme_light()
```

Figure 4 affirms that default rates varied over time, but, interestingly, different Borrower Regions' default rates were slightly different. The Northeast and Midwest appear to have been slighlty insulated from the worst of the GFC relative to the West and South.


## Project Region vs. Annual Default Rate

Project Region Specification
```{r}
raw_data$ProjectRegion[raw_data$ProjectState == "ME"| raw_data$ProjectState == "VT"| raw_data$ProjectState == "NH"|raw_data$ProjectState == "MA"| raw_data$ProjectState == "RI"| raw_data$ProjectState == "NY"| raw_data$ProjectState == "CT"| raw_data$ProjectState == "NJ"|raw_data$ProjectState == "PA"] = "Northeast"

raw_data$ProjectRegion[raw_data$ProjectState == "ND"| raw_data$ProjectState == "SD"| raw_data$ProjectState == "NE"|raw_data$ProjectState == "KS"| raw_data$ProjectState == "MN"| raw_data$ProjectState == "IA"| raw_data$ProjectState == "MO"| raw_data$ProjectState == "WI" | raw_data$ProjectState == "IL"| raw_data$ProjectState == "IN"| raw_data$ProjectState == "MI" | raw_data$ProjectState == "OH"] = "Midwest"

raw_data$ProjectRegion[raw_data$ProjectState == "TX"| raw_data$ProjectState == "OK"| raw_data$ProjectState == "AR"|raw_data$ProjectState == "LA"| raw_data$ProjectState == "MS"| raw_data$ProjectState == "AL"| raw_data$ProjectState == "TN"| raw_data$ProjectState == "KY"| raw_data$ProjectState == "WV"| raw_data$ProjectState == "VA"|raw_data$ProjectState == "MD"| raw_data$ProjectState == "DC"| raw_data$ProjectState == "DE"| raw_data$ProjectState == "NC"| raw_data$ProjectState == "SC"| raw_data$ProjectState == "GA" | raw_data$ProjectState == "FL" | raw_data$ProjectState == "PR" | raw_data$ProjectState == "VI"] = "South"

raw_data$ProjectRegion[raw_data$ProjectState == "WA"| raw_data$ProjectState == "OR"| raw_data$ProjectState == "CA"|raw_data$ProjectState == "NV"| raw_data$ProjectState == "ID"| raw_data$ProjectState == "MT"| raw_data$ProjectState == "UT"| raw_data$ProjectState == "WY"|raw_data$ProjectState == "CO"| raw_data$ProjectState == "NM"| raw_data$ProjectState == "AZ"| raw_data$ProjectState == "AK"| raw_data$ProjectState == "HI" | raw_data$ProjectState == "GU"] = "West"
```

Loan Volume by Project Region
```{r}
raw_data %>% 
  drop_na(ProjectRegion) %>% 
  group_by(ProjectRegion) %>% 
  summarise("Volume" = sum(GrossApproval)) %>% 
  ggplot(aes(x=ProjectRegion, y = Volume, fill = ProjectRegion)) + 
  geom_bar(stat="identity") + 
  ggtitle("Figure 5: Total Loan Volume by Project Region") + 
  xlab("Project Region") + 
  ylab("Total Loan Volume") 
```

Figure 5 shows that the dataset tilts towards loans from the West, South, and Midwest. 

```{r}
raw_data %>% 
  drop_na(ProjectRegion) %>%  
  group_by(ApprovalFiscalYear, ProjectRegion) %>% 
  summarise("Annual Default Rate" = sum(LoanStatus == "CHGOFF")/sum(LoanStatus == "CHGOFF" | LoanStatus == "PIF")) %>% 
  ggplot(
    aes(
      x = ApprovalFiscalYear,
      y = `Annual Default Rate`, 
      group = ProjectRegion, 
      color = ProjectRegion)
    ) + 
  geom_point() + 
  geom_line() + 
  ggtitle("Figure 6: Approval Year vs. Annual Default Rate by Project Region") + 
  xlab("Approval Year") + 
  ylab("Annual Default Rate")
```

Figure 6 mirrors Figure 4 - while all project regions experienced similar default paths over time, the project regions did have slightly different experiences. As in Figure 4, the Northeast and Midwest appear to have been slighlty insulated from the worst of the GFC relative to the West and South.

## Business Type vs. Annual Default Rate

Loan Volume by Business Type
```{r}
raw_data[!(raw_data$BusinessType == ""), ] %>% 
  drop_na(BusinessType) %>% 
  group_by(BusinessType) %>% 
  summarise("Volume" = sum(GrossApproval)) %>% 
  ggplot(aes(x = BusinessType, y = Volume, fill = BusinessType)) + 
  geom_bar(stat = "identity") + 
  ggtitle("Figure 7: Total Loan Volume by Business Type") + 
  xlab("Business Type") + 
  ylab("Total Loan Volume")
```

Figure 7 shows that Corporations were the primary borrowers in the SBA dataset; individuals and partnerships make up fairly small portions of the dataset's total loan volume.


```{r}
raw_data[!(raw_data$BusinessType == ""), ] %>% 
  group_by(ApprovalFiscalYear, BusinessType) %>% 
  summarise("Annual Default Rate" = sum(LoanStatus == "CHGOFF")/sum(LoanStatus == "CHGOFF" | LoanStatus == "PIF")) %>% 
  ggplot(
    aes(
      x = ApprovalFiscalYear, 
      y = `Annual Default Rate`, 
      group = BusinessType, 
      color = BusinessType)
    ) + 
  geom_point() + 
  geom_line() + 
  ggtitle("Figure 8: Approval Year vs. Annual Default Rate by Business Type") + 
  xlab("Approval Year") + 
  ylab("Annual Default Rate")
```
Figure 9 demonstrates that the GFC affected all business types. All borrowers - corporations, individuals, and partnerships - suffered greatly during the GFC.


## NAICS vs. Annual Default Rate
Annual Default Rate per Approval Year by NAICS Code (Subset 1)
```{r}
temp = raw_data
temp$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "11"] = "Agriculture"
temp$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "21"] = "Mining & Oil"
temp$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "22"] = "Utilities"
temp$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "23"] = "Construction"
temp$NAICS_Sector[substring(raw_data$NaicsCode, 1, 1) == "3"] = "Manufacturing"
temp$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "42"] = "Wholesale Trade"
temp$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "44" | substring(raw_data$NaicsCode, 1, 2) == "45"] = "Retail Trade"
temp$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "48" | substring(raw_data$NaicsCode, 1, 2) == "49"] = "Transportation & Warehousing"
temp$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "51"] = "Information"
temp$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "52"] = "Finance & Insurance"

temp %>% 
  drop_na(NAICS_Sector) %>% 
  group_by(ApprovalFiscalYear, NAICS_Sector) %>% 
  summarise(
    "Annual Default Rate" = sum(LoanStatus == "CHGOFF")/sum(LoanStatus == "CHGOFF" | LoanStatus == "PIF")
  ) %>% 
  ggplot(
  aes(
    x = ApprovalFiscalYear, 
    y = `Annual Default Rate`, 
    group = NAICS_Sector, color = NAICS_Sector)
  ) + 
  geom_point() + 
  geom_line() + 
  ggtitle("Figure 9: Approval Year vs. Annual Default Rate by NAICS Sector (Subset 1)") + 
  xlab("Approval Year") + 
  ylab("Annual Default Rate")
```


```{r}
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "53"] = "Real Estate & Leasing"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "54"] = "Professional Services"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "55"] = "Management Services"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "56"] = "Administrative and Waste Management Services"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "61"] = "Educational Services"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "62"] = "Health Care"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "71"] = "Arts & Entertainment"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "72"] = "Accommodation and Food Services"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "81"] = "Other Services"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "92"] = "Public Administration"

raw_data %>% 
  drop_na(NAICS_Sector) %>%  
  group_by(ApprovalFiscalYear, NAICS_Sector) %>% 
  summarise(
    "Annual Default Rate" = 
      sum(LoanStatus == "CHGOFF")/sum(LoanStatus == "CHGOFF" | LoanStatus == "PIF")
  ) %>% 
  ggplot(
    aes(x = ApprovalFiscalYear, 
        y = `Annual Default Rate`, 
        group = NAICS_Sector, 
        color = NAICS_Sector)
    ) + 
  geom_point() + 
  geom_line() + 
  ggtitle("Figure 10: Approval Year vs. Annual Default Rate by NAICS Sector (Subset 2)") + 
  xlab("Approval Year") + 
  ylab("Annual Default Rate")
```

Figures 9 and 10 are striking - the effect of the GFC was not at all equivalent between NAICS Sectors. Real Estate & Leasing, for example, experienced tremendous growth in default rates during the GFC while management services and wholesale trade were much more insulated from the worst of the crisis.

Total Loan Volume by NAICS Code
```{r}
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "11"] = "Agriculture"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "21"] = "Mining & Oil"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "22"] = "Utilities"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "23"] = "Construction"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 1) == "3"] = "Manufacturing"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "42"] = "Wholesale Trade"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "44" | substring(raw_data$NaicsCode, 1, 2) == "45"] = "Retail Trade"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "48" | substring(raw_data$NaicsCode, 1, 2) == "49"] = "Transportation & Warehousing"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "51"] = "Information"
raw_data$NAICS_Sector[substring(raw_data$NaicsCode, 1, 2) == "52"] = "Finance & Insurance"

raw_data %>% 
  drop_na(NAICS_Sector) %>% 
  group_by(NAICS_Sector) %>% 
  summarise("Volume" = sum(GrossApproval)) %>% 
  mutate(NAICS_Sector = fct_reorder(NAICS_Sector, Volume)) %>% 
  ggplot(aes(x = NAICS_Sector, y=Volume, color = NAICS_Sector)) + 
  geom_bar(stat = "identity") + 
  ggtitle("Figure 11: Total Loan Volume by NAICS Sector") + 
  xlab("NAICS Sector") + 
  ylab("Total Loan Volume") +
  coord_flip() +
  theme(legend.position = "none")
```

Figure 11 shows that total loan volume varies greatly with NAICS sector. In the SBA Data, the loan volumes of Retail Trade, Manufacturing, and Accommodation and Food Services dwarf those of Mining & Oil, Utilities, Management Services, and Public Administration.

```{r}
temp = raw_data
temp = transform(raw_data, bin = cut(TermInMonths, breaks= c(0, 119, 180, 240, 500)))
raw_data$TermLength[temp$bin == "(0,119]"] = "<120mths"
raw_data$TermLength[temp$bin == "(119,180]"] = "120-180mths"
raw_data$TermLength[temp$bin == "(180,240]"] = "181-240mths"
raw_data$TermLength[temp$bin == "(240,500]"] = "240mths+"

raw_data %>% 
  drop_na(TermLength) %>% 
  group_by(TermLength) %>% 
  summarise("Volume" = sum(GrossApproval)) %>% 
  ggplot(aes(x=TermLength, y=Volume, color = TermLength)) + 
  geom_bar(stat="identity") + 
  ggtitle("Figure 12: Total Loan Volume by Term Length") + 
  xlab("Term Length") + ylab("Total Loan Volume")
```

Figure 12 shows the tremendous concentration of loan volume in loans whose term lengths are between 180 and 240 months. Other term lengths are much less represented in the data.

```{r}
raw_data %>% 
  drop_na(TermLength) %>% 
  group_by(ApprovalFiscalYear, TermLength) %>% 
  summarise(
    "Annual Default Rate" = sum(LoanStatus == "CHGOFF")/sum(LoanStatus == "CHGOFF" | LoanStatus == "PIF")
  ) %>% 
  ggplot(
    aes(x=ApprovalFiscalYear, y= `Annual Default Rate`, group = TermLength, color = TermLength)) + 
  geom_point() + 
  geom_line() + 
  ggtitle("Approval Year vs. Annual Default Rate by Term Length") + 
  xlab("Approval Year") + 
  ylab("Annual Default Rate") 
```


# Pre-Processing the Data for Modeling

Data Cleaning - Dealing with states that are absent in the data and states in which there are no borrowers
```{r}
raw_data1 <- raw_data

raw_data1 %>% 
  select(ApprovalDate) %>% 
  mutate(date2 = as.Date(ApprovalDate, "%m/%d/%y"))
  
raw_data = raw_data[!(raw_data$LoanStatus=="CANCLD" | raw_data$LoanStatus=="EXEMPT"),]
#15 occurances of different states or NA thus delete those loans

unidentified.states = unique(raw_data$ProjectState[!(raw_data$ProjectState %in% unique(UnemploymentUSbyState$State))]) #States not in raw_data$ProjectState
raw_data = raw_data[!(raw_data$ProjectState %in% unidentified.states),] #remove those unidentified states from raw_data

#States not in raw_data$BorrState; aka, states in which there are no borrowers
unidentified.states.Borr = unique(raw_data$BorrState[!(raw_data$BorrState %in% unique(UnemploymentUSbyState$State))]) 

temp.data <- 
  raw_data %>% 
  mutate(
    ApprovalDate = as.Date(ApprovalDate),
    month.bin = cut(ApprovalDate, breaks = "month")
  )
```

## Addition of Unemployment Statistics at the State Level by Month 
```{r}
tempUR = rename(UnemploymentUSbyState, month.bin = DATE, ProjectState = State, URinProjectState = UR)
temp = merge(x=temp.data,y=tempUR,by=c("ProjectState","month.bin"))

tempUR.Borr = rename(UnemploymentUSbyState, month.bin = DATE, BorrState = State, URinBorrState = UR)
temp = merge(x=temp,y=tempUR.Borr,by=c("BorrState","month.bin"))
```

## Addition of GDP by Quarter
```{r}
tempGDP <- 
  transmute(GDP, month.bin = DATE, GDP = GDP) %>% 
  mutate(
    quarter.bin =
      case_when(
        as.numeric(substring(month.bin, 6, 7)) < 10 & as.numeric(substring(month.bin, 6, 7)) >= 7  ~ paste("Q3", substring(month.bin, 0, 4)),
        as.numeric(substring(month.bin, 6, 7)) < 7 & as.numeric(substring(month.bin, 6, 7)) >= 4   ~ paste("Q2", substring(month.bin, 0, 4)),
        as.numeric(substring(month.bin, 6, 7)) < 4                                                 ~ paste("Q1", substring(month.bin, 0, 4)),
        TRUE                                                                                       ~ paste("Q4", substring(month.bin, 0, 4)),
      )
  )

temp <- 
  temp %>% 
  mutate(
    quarter.bin =
      case_when(
        as.numeric(substring(ApprovalDate, 6, 7)) < 10 & as.numeric(substring(ApprovalDate, 6, 7)) >= 7  ~ paste("Q3", substring(ApprovalDate, 0, 4)),
        as.numeric(substring(ApprovalDate, 6, 7)) < 7 & as.numeric(substring(ApprovalDate, 6, 7)) >= 4   ~ paste("Q2", substring(ApprovalDate, 0, 4)),
        as.numeric(substring(ApprovalDate, 6, 7)) < 4                                                    ~ paste("Q1", substring(ApprovalDate, 0, 4)),
        TRUE                                                                                             ~ paste("Q4", substring(ApprovalDate, 0, 4)),
      )
  ) %>% 
  merge(y = tempGDP,by="quarter.bin") %>% 
  subset(select = -quarter.bin) %>% 
  subset(select = -month.bin.y) %>% 
  rename("month.bin" = month.bin.x)
```

## Addition of S&P 500 Data by Month
```{r}
tempSP500 = as.data.frame(SP500)
tempSP500$month.bin = rownames(tempSP500)
tempSP500 = transmute(tempSP500,  month.bin = rownames(tempSP500), GSPC.price= GSPC.Adjusted)
temp = merge(x = temp, y = tempSP500,by = "month.bin")
```

## Addition of Fed Funds by Month
```{r}
tempFedFunds = transmute(FEDFUNDS, month.bin = DATE, FedFunds = FEDFUNDS)
temp = merge(x=temp,y=tempFedFunds,by="month.bin")
```


## Addition of CPI by Month
```{r}
tempCPI = transmute(CPI, month.bin = DATE, CPI = CPIAUCSL)
temp = merge(x = temp, y = tempCPI,by = "month.bin")
raw_data = subset(temp, select = -month.bin)
```

## Addition of a Binary Term for Loans whose term is evenly divisible by 12 - BinaryIntergerTerm
```{r}
raw_data$BinaryIntergerTerm[raw_data$TermInMonths %% 12 == 0] = 1
raw_data$BinaryIntergerTerm[raw_data$TermInMonths %% 12 != 0] = 0
```

## Addition of a Binary term for repeat borrowers - BinaryRepeatBorrower
```{r}
temp = duplicated(raw_data$BorrName)
raw_data$BinaryRepeatBorrower[temp == TRUE] = 1 
raw_data$BinaryRepeatBorrower[temp == FALSE] = 0
```

## Addition of a Binary Term for Loans that have accepted Third Party Dollars - BinaryThirdPartyDollars
```{r}
#Not sure what to do with the NAs here. The variable ThirdPartyDollars only takes on values TRUE and NA, so I've encoded the NAs as FALSE here.
raw_data <- 
  raw_data %>% 
  mutate(
    BinaryThirdPartyDollars = 
      case_when(ThirdPartyDollars ~ 1, !ThirdPartyDollars ~ 0, TRUE ~ as.double(NA))
  )
```

## Addition of a Binary Term for Loans in which the Bank's State Matches the Borrower's State - BinaryBankStEqualBorrowerSt
```{r}
raw_data$BinaryBankStEqualBorrowerSt[as.character(raw_data$BorrState) == as.character(raw_data$CDC_State)] = 1
raw_data$BinaryBankStEqualBorrowerSt[as.character(raw_data$BorrState) != as.character(raw_data$CDC_State)] = 0
```

##BinaryProjectStEqualBorrowerSt
```{r}
# QUESTION: what does this do??
raw_data$BinaryProjectStEqualBorrowerSt[as.character(raw_data$BorrState) == as.character(raw_data$ProjectState)] = 1
raw_data$BinaryProjectStEqualBorrowerSt[as.character(raw_data$BorrState) != as.character(raw_data$ProjectState)] = 0
```

## Modifying Key Continous Variables to Log form: "LogGrossApproval", "TermInMonths", "LogGDP", "LogSP500", "LogFedFunds", "LogCPI", "URinProjectState", "URinBorrState" (NOTE: "LogThirdPartyDollars" has been removed. Not continuous - logical variable. Created BinaryThirdPartyDollars instead)
```{r}
raw_data$LogGrossApproval = log(raw_data$GrossApproval)
raw_data$LogGDP = log(raw_data$GDP)
raw_data$LogSP500 = log(raw_data$GSPC.price)
raw_data$LogFedFunds = log(raw_data$FedFunds)
raw_data$LogCPI = log(raw_data$CPI)
```

## Stitching Together Model Data and Adding Binary Default Variable: "BusinessType", "NAICS_Sector", "DeliveryMethod", "BinaryIntergerTerm", "BinaryRepeatBorrower", "BinaryBankStEqualBorrowerSt", "BinaryProjectStEqualBorrowerSt", "ApprovalFiscalYear", "BorrowerRegion", "ProjectRegion", "BinaryThirdPartyDollars"

```{r}
modified_data = raw_data[c("LogGrossApproval", "BinaryThirdPartyDollars", "TermInMonths", "LogGDP", "LogSP500", "LogFedFunds", "LogCPI", "URinProjectState", "URinBorrState", "BusinessType", "NAICS_Sector", "DeliveryMethod", "BinaryIntergerTerm", "BinaryRepeatBorrower", "BinaryBankStEqualBorrowerSt", "BinaryProjectStEqualBorrowerSt", "ApprovalFiscalYear", "BorrowerRegion", "ProjectRegion")]

modified_data$Default[raw_data$LoanStatus == "CHGOFF"] = 1
modified_data$Default[raw_data$LoanStatus != "CHGOFF"] = 0
```

## Normalization
```{r}
c = 0.1

modified_data <- 
  modified_data %>% 
  mutate(
    across(
      .cols = 
        c(
          LogGrossApproval, 
          TermInMonths,
          LogGDP,
          LogSP500,
          LogFedFunds,
          LogCPI,
          URinProjectState,
          URinBorrState
        ),
      .fns = ~ (. - mean(., na.rm = TRUE)) / (c + sd(., na.rm = TRUE))
    )
  )
```

## Dealing with Missing Values
### Continous Variables
```{r}
temp = modified_data[c("LogGrossApproval", "TermInMonths", "LogGDP", "LogSP500", "LogFedFunds", "LogCPI", "URinProjectState", "URinBorrState")]
temp[is.na(temp)] = 0
modified_data[c("LogGrossApproval", "TermInMonths", "LogGDP", "LogSP500", "LogFedFunds", "LogCPI", "URinProjectState", "URinBorrState")] = temp
```

### Discrete Variables
```{r}
temp = modified_data[c("BusinessType", "NAICS_Sector", "DeliveryMethod", "BinaryIntergerTerm", "BinaryRepeatBorrower", "BinaryBankStEqualBorrowerSt", "BinaryProjectStEqualBorrowerSt", "ApprovalFiscalYear", "BorrowerRegion", "ProjectRegion", "BinaryThirdPartyDollars")]
temp[is.na(temp)] = "Blank"
modified_data[c("BusinessType", "NAICS_Sector", "DeliveryMethod", "BinaryIntergerTerm", "BinaryRepeatBorrower", "BinaryBankStEqualBorrowerSt", "BinaryProjectStEqualBorrowerSt", "ApprovalFiscalYear", "BorrowerRegion", "ProjectRegion", "BinaryThirdPartyDollars")] = temp
```

## Data Paritioning
```{r}
train_size = round((nrow(modified_data)/10)*7, 0)
validation_size = round((nrow(modified_data)/10), 0)
test_size = nrow(modified_data) - train_size - validation_size

train_data = modified_data[0:train_size,]
validation_data = modified_data[(train_size+1):(train_size+validation_size),]
test_data = modified_data[(train_size+validation_size+1):nrow(modified_data),]
```

# LOGISTIC MODEL

## Creating the Basic Logistic Model

### Training the Model

```{r}
# log_model = glm(data = train_data, Default ~., family = binomial)
# summary(log_model)
# 
# #Training ROC AUC
# log_model_train_prediction = predict(log_model, train_data, type = "response")
# log_model_train_prediction = prediction(log_model_train_prediction, train_data$Default)
# auc = unlist(slot(performance(log_model_train_prediction, 'auc'), 'y.values'))
# auc
# #0.7256392
# 
# #Test ROC AUC
# log_model_test_prediction = predict(log_model, newdata = test_data, type="response") # test_data has entry "BLANK" for BusinessType var.
# temp = test_data$Default[!is.na(log_model_test_prediction)]
# log_model_test_prediction = log_model_test_prediction[!is.na(log_model_test_prediction)]
# log_model_test_prediction = prediction(log_model_test_prediction, temp)
# auc = unlist(slot(performance(log_model_test_prediction, 'auc'), 'y.values'))
# auc
# #0.6053772
# 
# #Plotting ROCs
# roc_train = performance(log_model_train_prediction,"tpr","fpr")
# roc_test = performance(log_model_test_prediction,"tpr","fpr")
# plot(roc_train, col = 'red', main = 'Basic Logistic Model Training ROC (red) vs. Testing ROC (blue)')
# plot(roc_test, add = TRUE, col = 'blue')
# abline(a = 0, b = 1)
```

## Ridge and Lasso Logisitic Model

### Training

```{r}
x_train = model.matrix(Default ~., train_data)[, -1]
x_train = x_train[,order(colnames(x_train))]
y_train = train_data$Default
```

```{r}
setdiff(unlist(attr(x_train, "dimnames")[2]), unlist(attr(x_validation, "dimnames")[2]))
```

```{r}
model_L1 = glmnet(x_train, y_train, alpha = 1, nlambda = 10, family = "binomial") #Lasso Penalty -> Why is nlambda = 10? We have 20 features, shouldn't we have one lambda per beta?
model_L2 = glmnet(x_train, y_train, alpha = 0, nlambda = 10, family = "binomial") #Ridge Penalty -> Same question as above

x_validation = model.matrix(Default ~., validation_data)[, -1]
x_validation = x_validation[,order(colnames(x_validation))]

prediction_L1_train = predict(model_L1, newx = x_train, type = "response")
prediction_L2_train = predict(model_L2, newx = x_train, type = "response")
prediction_L1_validation = predict(model_L1, newx = x_validation, type = "response")
prediction_L2_validation = predict(model_L2, newx = x_validation, type = "response")
```

### Validation: Tuning Lambda Hyperparameter
```{r}
AUC_L1_train = vector()
AUC_L2_train = vector()
AUC_L1_validation = vector()
AUC_L2_validation = vector()
for(i in 1:10){
  AUC_L1_train = append(AUC_L1_train, unlist(slot(performance(prediction(prediction_L1_train[,i], train_data$Default), 'auc'), 'y.values')))
  AUC_L2_train = append(AUC_L2_train, unlist(slot(performance(prediction(prediction_L2_train[,i], train_data$Default), 'auc'), 'y.values')))
  AUC_L1_validation = append(AUC_L1_validation, unlist(slot(performance(prediction(prediction_L1_validation[,i], validation_data$Default), 'auc'), 'y.values')))
  AUC_L2_validation = append(AUC_L2_validation,  unlist(slot(performance(prediction(prediction_L2_validation[,i], validation_data$Default), 'auc'), 'y.values')))
}
```


### Best L1 Hyperparameter
```{r}
best_L1_lambda_index = which.max(AUC_L1_validation)
best_L1_lambda = model_L1$lambda[best_L1_lambda_index]
best_L1_AUC = max(AUC_L1_validation)
best_L1_AUC
#0.6699623
best_model_L1_coef = as.matrix(coef(model_L1, s= model_L1$lambda[best_L1_lambda_index]))
```

#Best L2 Hyperparameter
```{r}
best_L2_lambda_index = which.max(AUC_L2_validation)
best_L2_lambda = model_L2$lambda[best_L2_lambda_index]
best_L2_AUC = max(AUC_L2_validation)
best_L2_AUC
#0.6719302
best_model_L2_coef = as.matrix(coef(model_L2, s= model_L2$lambda[best_L2_lambda_index]))
```

#Plotting best L1 & L2 coefficients
```{r}
best_L1_model_coeff_plot = qplot(y= best_model_L1_coef[,1])
best_L1_model_coeff_plot + labs(title = "L1 Logistic Model Coeffcients", x= "Covariates", y= "Coeffcient Value")
best_L2_model_coeff_plot = qplot(y= best_model_L2_coef[,1])
best_L2_model_coeff_plot + labs(title = "L2 Logistic Model Coeffcients", x= "Covariates", y= "Coeffcient Value")
```

#Testing: best L2 model
```{r}
x_test = model.matrix(Default ~., test_data)[, -1]
x_test = x_test[,order(colnames(x_test))]

prediction_L2_test = predict(model_L2, newx = x_test, s = best_L2_lambda,  type = "response")
prediction_L2_test = prediction(prediction_L2_test, test_data$Default)
auc = unlist(slot(performance(prediction_L2_test, 'auc'), 'y.values'))
auc
#0.5648184
```

#Plotting ROCs
```{r}
roc_train = performance(prediction(prediction_L2_train[,best_L2_lambda_index], train_data$Default),"tpr","fpr")
roc_validation = performance(prediction(prediction_L2_validation[,best_L2_lambda_index], validation_data$Default),"tpr","fpr")
roc_test = performance(prediction_L2_test,"tpr","fpr")
plot(roc_train, col = 'red', main = 'L2 Logistic Model Training ROC (red) vs. Validation ROC (green) vs. Testing ROC (blue)')
plot(roc_validation, add = TRUE, col = 'green')
plot(roc_test, add = TRUE, col = 'blue')
abline(a = 0, b = 1) 
```

# Neural Net
## Data Partitioning

```{r}
#TEMPORARY - For now, import the most up-to-date data

modified_data = read.csv("modified_data.csv", header = TRUE)
```


```{r}
train_size = round((nrow(modified_data)/10)*7, 0)
validation_size = round((nrow(modified_data)/10), 0)
test_size = nrow(modified_data) - train_size - validation_size

train_data = modified_data[0:train_size,]
validation_data = modified_data[(train_size+1):(train_size+validation_size),]
test_data = modified_data[(train_size+validation_size+1):nrow(modified_data),]
```

#Setup
```{r}
set.seed(1)
softplus = function(x) {log(1 + exp(x))}
sigmoid = function(x) {1/(1+ exp(-x))}
swish = function(x) {x*sigmoid(x)}
```

#Training: Default hidden layers c(4,2) & sigmoid & rep = 3
```{r}
train_data_2 = train_data[sample(nrow(train_data), 5000), ]
nn = neuralnet(Default ~., data = train_data_2, hidden= c(4,2), rep = 3, act.fct = sigmoid, linear.output = FALSE)

plot(nn)
nn_train_prediction = compute(nn,train_data)
nn_train_prediction = as.vector(nn_train_prediction$net.result)
nn_train_prediction = prediction(nn_train_prediction, train_data$Default)
auc = unlist(slot(performance(nn_train_prediction, 'auc'), 'y.values'))
auc
#0.6615994
```


#Validation: Tuning architecture (Hidden Layer Count, Hidden Variables, act. fun)
act_functions = c('logistic', 'tanh', softplus, sigmoid, swish)
hidden_variables = c(2, 4, 8, 16, 32)
hidden_layers = c(1, 2, 3, 4, 5)

#Validation: Act. Function with Default c(4,2) for hidden layer
AUC_Act_Fn = vector()
for(i in 1:5){
  nn = neuralnet(Default ~., data = train_data_2, hidden= c(4,2), act.fct = act_functions[i], linear.output = FALSE)
  nn_validation_prediction = compute(nn,validation_data)
  nn_validation_prediction = as.vector(nn_validation_prediction$net.result)
  nn_validation_prediction = prediction(nn_validation_prediction, validation_data$Default)
  AUC_Act_Fn = append(AUC_Act_Fn, unlist(slot(performance(nn_validation_prediction, 'auc'), 'y.values')))
}

best_Act_Fn_index = which.max(AUC_Act_Fn)
best_Act_Fn_AUC = max(AUC_Act_Fn)
best_Act_Fn_AUC
#...

#Validation: Hidden Variables  with Default 2 hidden layers & best Act Fn
AUC_Hidden_Variables = vector()
for(i in 1:5){
  nn = neuralnet(Default ~., data = train_data_2, hidden= c(AUC_Hidden_Variables[i],AUC_Hidden_Variables[i]/2), act.fct = act_functions[best_Act_Fn_index], linear.output = FALSE)
  nn_validation_prediction = compute(nn,validation_data)
  nn_validation_prediction = as.vector(nn_validation_prediction$net.result)
  nn_validation_prediction = prediction(nn_validation_prediction, validation_data$Default)
  AUC_Hidden_Variables = append(AUC_Hidden_Variables, unlist(slot(performance(nn_validation_prediction, 'auc'), 'y.values')))
}

best_Hidden_Variables_index = which.max(AUC_Hidden_Variables)
best_Hidden_Variables_AUC = max(AUC_Hidden_Variables)
best_Hidden_Variables_AUC
#...

#Validation: Hidden Layers with best hidden variables & best Act Fn
#Need to manually build hidden layer structure given hidden variable output
AUC_Hidden_Layers = vector()
for(i in 1:5){
  nn = neuralnet(Default ~., data = train_data_2, hidden= c(AUC_Hidden_Variables[best_Hidden_Variables_index],AUC_Hidden_Variables[i]/2), act.fct = act_functions[best_Act_Fn_index], linear.output = FALSE)
  nn_validation_prediction = compute(nn,validation_data)
  nn_validation_prediction = as.vector(nn_validation_prediction$net.result)
  nn_validation_prediction = prediction(nn_validation_prediction, validation_data$Default)
  AUC_Hidden_Layers = append(AUC_Hidden_Layers, unlist(slot(performance(nn_validation_prediction, 'auc'), 'y.values')))
}

best_Hidden_Layers_index = which.max(AUC_Hidden_Layers)
best_Hidden_Layers_AUC = max(AUC_Hidden_Layers)
best_Hidden_Layers_AUC
#...
